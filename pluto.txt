MULTI(with pca):
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
X=data.drop(['Y house price of unit area'],axis=1)
y=data['Y house price of unit area']
print(X.shape)
print(y.shape)
from sklearn.preprocessing import StandardScaler
scaled=StandardScaler()
x_scaled=scaled.fit_transform(X)
from sklearn.decomposition import PCA
pca=PCA(n_components=0.85)
x_pca=pca.fit_transform(x_scaled)
print("Before PCA:",X.shape,"\nAfter PCA:",x_pca.shape)
X_train,X_test,y_train,y_test=train_test_split(x_pca,y,test_size=0.3)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
linreg=LinearRegression()
linreg.fit(X_train,y_train)
y_pred=linreg.predict(X_test)
y_pred
print("Mean squared error {}".format(mean_squared_error(y_test,y_pred)))
print("R2 score {}".format(r2_score(y_test,y_pred)))


LOGISTICS REG(WITH PCA SPAMHAM):
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
data=pd.read_csv('spam_ham_dataset.csv')
data[data.isnull().any(axis=1)]
from sklearn.preprocessing import LabelEncoder
encode=LabelEncoder()
data['label']=encode.fit_transform(data['label'])
data['text']=encode.fit_transform(data['text'])
X=data.drop(['label_num'],axis=1)
y=data['label_num']
from sklearn.preprocessing import StandardScaler
scaled=StandardScaler()
x_scaled=scaled.fit_transform(X)
from sklearn.decomposition import PCA
pca=PCA(n_components=0.85)
x_pca=pca.fit_transform(x_scaled)
print("Before PCA",X.shape,"After PCA",x_pca.shape)
x_train,x_test,y_train,y_test=train_test_split(x_pca,y,test_size=0.3)
logreg=LogisticRegression()
logreg.fit(x_train,y_train)
y_pred=logreg.predict(x_test)
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,r2_score,mean_squared_errorcm=confusion_matrix(y_test,y_pred)
print(cm)
tp=cm[0][0]
fp=cm[0][1]
fn=cm[1][0]
tn=cm[1][1]
acc=(tp+tn)/(tp+tn+fp+fn)
print(acc)
print("mse is {}".format(mean_squared_error(y_test,y_pred)))
print("r2 score is {}".format(r2_score(y_test,y_pred)))
from sklearn.metrics import classification_report
report=classification_report(y_test,y_pred) 
print(report)

LOGISTICS REG(with pca creditcard):
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA 
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.metrics import f1_score,confusion_matrix, classification_report 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import numpy as np
X = df.drop(['Class'], axis=1)
y=df['Class']
X = X.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)
y = y.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)
scaler = StandardScaler() 
X_scaled = pd.DataFrame(scaler.fit_transform(X)) 
X_scaled_drop = X_scaled.drop(X_scaled.columns[[2, 3, 12, 13, 22, 23]], axis=1)
pca = PCA(n_components=0.95) 
x_pca = pca.fit_transform(X_scaled_drop) 
x_pca = pd. DataFrame(x_pca)

print("Before PCA, X dataframe shape = ",X. shape, "\nAfter PCA, x_pca dataframe shape = ",x_pca.shape)
print(pca.explained_variance_ratio_) 
print(pca.explained_variance_ratio_.sum())
print(x_pca.shape) 
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.25, random_state=0) 
logpred = LogisticRegression() 
logpred.fit(X_train, y_train) 
y_pred =logpred.predict(X_test)
from sklearn.metrics import f1_score
print('f1_score : ',f1_score(y_test,y_pred,average='micro'))
cm = confusion_matrix(y_test, y_pred) 
print("Confusion matrix: \n", cm) 
report = classification_report(y_test, y_pred) 
print("Classification report: \n", report)


SVM(with pca):
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA 
from sklearn.svm import SVC 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn.metrics import confusion_matrix, classification_report 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import numpy as np
df=pd.read_csv('data.csv')
df.head()
df[df.duplicated()].shape
X=df.iloc[:,2:31].values
Y = df.iloc[:,1].values
corr_var=df.corr()
print(corr_var)
plt.figure(figsize=(20,17.5))
sns.heatmap(corr_var, annot=True, cmap='BuPu')
scaler = StandardScaler() 
X_scaled = pd.DataFrame(scaler.fit_transform(X)) 
X_scaled_drop = X_scaled.drop(X_scaled.columns[[2, 3, 12, 13, 22, 23]], axis=1)
pca = PCA(n_components=0.95) 
x_pca = pca.fit_transform(X_scaled_drop) 
x_pca = pd. DataFrame(x_pca)

print("Before PCA, X dataframe shape = ",X. shape, "\nAfter PCA, x_pca dataframe shape = ",x_pca.shape)
print(pca.explained_variance_ratio_) 
print(pca.explained_variance_ratio_.sum())
print(x_pca.shape) 
print(Y.shape)
X_train, X_test, y_train, y_test = train_test_split(x_pca, Y, test_size=0.25, random_state=0) 
svc = SVC() 
svc.fit(X_train, y_train) 
y_pred =svc.predict(X_test)
cm = confusion_matrix(y_test, y_pred) 
print("Confusion matrix: \n", cm) 
report = classification_report(y_test, y_pred) 
print("Classification report: \n", report)

SVM(without pca):
le=LabelEncoder()
data['diagnosis']=le.fit_transform(data['diagnosis'])
data=data.drop(['Unnamed: 32'],axis=1)
X=data.drop(['diagnosis'],axis=1)
y=data['diagnosis']
print(X.shape)
print(y.shape)
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
svm=SVC()
svm.fit(x_train,y_train)
svm.score(x_test,y_test)
y_pred=svm.predict(x_test)
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix
print(accuracy_score(y_test,y_pred))
print(precision_score(y_test,y_pred,zero_division=1))
print(confusion_matrix(y_test,y_pred))

KMEANS(IRIS):
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import numpy as np
df.drop_duplicates(inplace=True)
sns.heatmap(df.corr(),annot=True)
X = df.iloc[:, 0:4].values
X
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)
    kmeans.fit(X)
    # inertia method returns wcss for that model
    wcss.append(kmeans.inertia_)
plt.figure(figsize=(10,5))
sns.lineplot(range(1, 11), wcss,marker='o',color='red')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0,n_init=10)
y_kmeans = kmeans.fit_predict(X)
plt.scatter(X[y_kmeans==0, 0], X[y_kmeans==0, 1], s=100, c='red', label ='Iris Setosa')
plt.scatter(X[y_kmeans==1, 0], X[y_kmeans==1, 1], s=100, c='blue', label ='Iris-versicolour')
plt.scatter(X[y_kmeans==2, 0], X[y_kmeans==2, 1], s=100, c='green', label ='Iris-virginica')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='yellow', label = 'Centroids',marker=',')
plt.title('Clusters of Customers')
plt.legend()
plt.show() 
df["species"] = pd.Categorical(df["species"])
df["species"] = df["species"].cat.codes
from sklearn.metrics import classification_report
target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
print(classification_report(df['species'],kmeans.labels_,target_names=target_names))

KMEANS(AGE INCOME):
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
X=data[['Age']]
y=data['Income']
k=KMeans(n_clusters=3)
k.fit(X,y)
prediction=k.fit_predict(data[['Age','Income']])
prediction
data['cluster']=prediction
k.cluster_centers_

df1=data[data.cluster==0]
df2=data[data.cluster==1]
df3=data[data.cluster==2]
plt.scatter(df1['Age'],df1['Income'],color='red',label='Cluster 0')
plt.scatter(df2['Age'],df2['Income'],color='black',label='Cluster 1')
plt.scatter(df3['Age'],df3['Income'],color='orange',label='Cluster 2')
plt.scatter(k.cluster_centers_[:,0],k.cluster_centers_[:,1],color='purple',marker='^',label='centroid')
sum_square_error=[]
k_range=range(1,10)
for k in k_range:
    k=KMeans(n_clusters=k)
    k.fit(data[['Age','Income']])
    sum_square_error.append(k.inertia_)
plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_range,sum_square_error)

NAIVE BAYES small covid:
from sklearn.model_selection import train_test_split, cross_val_score 
import matplotlib.pyplot as plt 
import seaborn as sns 
import pandas as pd 
import numpy as np 
df=pd.read_csv('/content/covid_dataset.csv')
df
df.columns
x=df.drop(labels=['Result'],axis=1)
y=df['Result']
from sklearn.naive_bayes import GaussianNB
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size= 0.25,random_state=355)
model = GaussianNB()
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_pred)  
print(cm)
true_positive = cm[0][0]
false_positive = cm[0][1]
false_negative = cm[1][0]
true_negative = cm[1][1]
Accuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)
print('accuracy is:',Accuracy)

Precision = true_positive/(true_positive+false_positive)
print('precision is:',Precision)

Recall = true_positive/(true_positive+false_negative)
print('recall is:',Recall)

F1_Score = 2*(Recall * Precision) / (Recall + Precision)
print('f1_score is:',F1_Score)
from sklearn.metrics import classification_report
report=classification_report(y_test,y_pred)
print(report)

NAIVE BAYES BIGCOVID:
data=data.drop(labels=['Deaths / 100 Recovered'], axis=1)
encode=LabelEncoder()
data.columns
data['Country/Region']=encode.fit_transform(data['Country/Region'])
data['WHO Region']=encode.fit_transform(data['WHO Region'])
X = data.drop(labels=['New deaths'],axis=1)
y = data['New deaths']
from sklearn.naive_bayes import GaussianNB
x_train,x_test,y_train,y_test = train_test_split(X,y, test_size= 0.25,random_state=355)

model = GaussianNB()
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
print(accuracy_score(y_test, y_pred))
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_pred)  

print(cm)
true_positive = cm[0][0]
false_positive = cm[0][1]
false_negative = cm[1][0]
true_negative = cm[1][1]
Accuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)
print('accuracy is:',Accuracy)

Precision = true_positive/(true_positive+false_positive)
print('precision is:',Precision)

Recall = true_positive/(true_positive+false_negative)
print('recall is:',Recall)

F1_Score = 2*(Recall * Precision) / (Recall + Precision)
print('f1_score is:',F1_Score)
from sklearn.metrics import confusion_matrix, classification_report 
print(classification_report(y_test,y_pred))


KNN:
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score 
import matplotlib.pyplot as plt 
import seaborn as sns 
import pandas as pd 
import numpy as np 
iris = load_iris()
X = iris.data 
y = iris.target 
 
data=np.c_[iris.data, iris.target] 
columns = np.append(iris.feature_names, ["target"]) 
df = pd. DataFrame(data, columns=columns) 
print(df) 
print(iris.feature_names)
df[iris.feature_names].describe()

sns.heatmap(df[iris.feature_names].corr(), annot=True) 
plt.plot()
sns.boxplot(x="target",y="petal length (cm)",data=df)
sns.boxplot(x="target",y="petal width (cm)",data=df)
sns.pairplot(df, hue="target", size=4)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state = 4) 
print(X_train.shape) 
print(X_test.shape)
print(y_train.shape) 
print(y_test.shape)
from sklearn.neighbors import KNeighborsClassifier 
from sklearn import metrics

test_k = range(1,26) 
scores = [] 
for k in test_k:
    knn = KNeighborsClassifier(n_neighbors=k) 
    knn.fit(X_train,y_train) 
    y_pred = knn.predict(X_test) 
    scores.append(metrics.accuracy_score(y_test,y_pred))

plt.plot(test_k, scores) 
plt.xlabel('k value for KNN') 
plt.ylabel('Accuracy on test data')
cv_scores = [] 
for k in test_k:
   knn = KNeighborsClassifier(n_neighbors=k) 
   score = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy') 
   cv_scores.append(score.mean())

MSE = [1 - x for x in cv_scores]
plt.title('The optimal number of neighbors') 
plt.xlabel('Number of Neighbors k') 
plt.ylabel('Misclassification Error') 
plt.plot(test_k, MSE)
plt.show()
knn = KNeighborsClassifier(n_neighbors=12)
knn.fit(X_train,y_train) 
y_pred = knn.predict(X_test) 
print("accuracy is:",metrics.accuracy_score(y_test,y_pred)*100)
from sklearn.metrics import confusion_matrix
cm=metrics.confusion_matrix(y_test,y_pred) 
print(cm)
from sklearn.metrics import classification_report
report=metrics.classification_report(y_test,y_pred) 
print(report)

WEIGHTED:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
def kernel(point, xmat, k):
    m , n = np.shape(xmat)
    weights = np.mat(np.eye((m)))
    for j in range(m):
       diff = point - X[j]
       weights[j,j] = np.exp(diff*diff.T/(-2.0*k**2))
    return weights
def localWeight(point, xmat, ymat, k):
    wei = kernel(point,xmat,k)
    W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T))
    return W
def localWeightRegression(xmat, ymat, k):
    m,n = np.shape(xmat)
    ypred = np.zeros(m)
    for i in range(m):
        ypred[i] = xmat[i]*localWeight(xmat[i],xmat,ymat,k)
    return ypred
data = pd.read_csv('/content/Restaurant_dataset.csv')
bill = np.array(data.total_bill)
tip = np.array(data.tip)
mbill = np.mat(bill)
mtip = np.mat(tip)
m= np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T,mbill.T))
ypred = localWeightRegression(X,mtip,0.5)
SortIndex = X[:,1].argsort(0)
xsort = X[SortIndex][:,0]
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.scatter(bill,tip, color='green')
ax.plot(xsort[:,1],ypred[SortIndex], color = 'red', linewidth=5)
plt.xlabel('Total bill')
plt.ylabel('Tip')
plt.show();

